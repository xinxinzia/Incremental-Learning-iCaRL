# Incremental-Learning-iCaRL

## 实验背景

增量学习（Incremental Learning）是一种机器学习方法，旨在让模型能够随着时间的推移不断学习新知识，而无需遗忘之前学过的知识。这种方法在处理数据持续变化的场景中尤为重要，例如在动态环境下的智能系统中。iCaRL（Incremental Classifier and Representation Learning）是一种常用的增量学习算法，能够在不访问旧数据的情况下，逐步增加新的类别。

## 实验目标

本实验目的在于手动撰写代码复现论文《iCaRL: Incremental Classifier and Representation Learning》中的 iCaRL 算法，并对原论文中涉及到的的核心算法内容进行一定的修改与探索。最后，通过对比三种不同类别增量策略来反映不同策略对于模型训练和分类准确率的影响，以及侧面反映该算法对于“灾难性遗忘”的缓解效果。通过实验，更深刻地理解类增量学习的深度学习方法框架以及其中应用到的具体优化方法细节。

## 建模过程

本实验的目标是实现并评估 iCaRL 算法，具体包括以下步骤：

1. **数据准备**：
   - 数据分割：使用自定义的 data_split1.split_data 函数将数据分割为初始训练集、测试集和增量训练集、测试集。使用torchvision 中的transforms 对训练和测试数据进行归一化和增强
   - 数据增强：对训练数据调整像素大小，随机水平翻转，转换为张量，并进行归一化处理。对测试数据调整大小，转换为张量，并进行归一化处理。

2. **训练阶段**：
   - 初始训练阶段：
     1. 加载预训练的ResNet50 模型并重新定义全连接层。
     2. 定义损失函数（交叉熵损失的负对数似然损失）和优化器（Adam）。
     3. 使用自定义的 train_and_valid 函数进行训练和验证，并保存模型参数和训练结果。
   - 增量训练阶段：
     1. 根据已训练模型特征层，通过计算欧几里得距离，挑选上一个任务训练数据集各图像类别中与模型特征最接近的图像，将其作为旧类别样本加入到记忆缓冲区中。
     2. 将记忆缓冲区中的样本与增量训练数据合并。
     3. 迁移上一个任务所保存的训练模型，使用自定义的train_and_valid 函数进行进一步的训练和验证，并保存当前模型的参数和训练结果。
     4. 在每个增量任务结束后，冻结已训练过的全连接层的参数。

3. **实验评估**：
   - 评估模型在每个增量阶段的分类准确率，并与传统方法进行对比。
   - 分析模型在处理新类别时的性能变化，以及在保留旧类别知识方面的效果。

## 实验探索

1. **探索 1：修改预训练模型**：
   - 将iCaRL算法中使用的预训练模型由原始论文中的ResNet32 修改为 ResNet50。
   - ResNet50 是一种更深的卷积神经网络，包含 50 层，相较于ResNet32 的 32 层，可以捕捉到更复杂和更高级别的特征表示。相较于ResNet32，ResNet50 由于层数更多，有更高的特征提取能力，可以更好地拟合数据的复杂分布，提升准确性，同时具有更佳的泛化能力。

2. **探索 2：修改损失函数**：
   - 将iCaRL算法中使用的损失函数由交叉熵损失函数修改为负对数似然损失函数。
   - 相较于交叉熵损失函数，负对数似然损失函数在处理多分类问题时具有良好的适用性和性能表现，同时它与概率和统计的联系更加紧密，有助于理解模型的训
练过程。

## 实验内容

本实验中，对于CIFAR100 数据集，我们共设计了三种增量策略，将数据集的100个类别分成多个类别增量任务，每一种策略的参数设置如下表所示：

|策略|base|增量方式|迭代轮次|学习率|蒸馏温度|抽取旧数据数量|训练批次大小|
|:---|:---|:---|:---|:---|:---|:---|:---|
|1|0|10类 * 10tasks|20|0.01|7|50|128|
|2|0|5类 * 20tasks|10|0.01|7|50|128|
|3|50|10类 * 5tasks|40|0.01|7|50|128|

实验中，将模型训练的学习率固定为0.01，知识蒸馏算法的蒸馏温度固定为7，从旧类别抽取旧数据的数量固定为50，训练批次固定为128。

**实验策略**：
   - 策略一：初始类别数为0，共有10个增量任务，每个任务新增10个类别，设置迭代轮次为20轮。
   - 策略二：初始类别数为0，共有20个增量任务，每个任务新增5个类别，设置迭代轮次为10轮。
   - 策略三：初始类别数为50，共有5个增量任务，每个任务新增10个类别，设置迭代轮次为40轮。

实验过程中，我们分别对这三种策略进行数据划分、依次增量、训练与测试等实验
步骤。

## 代码运行

### 数据准备
-将cifar100数据集下载下来并保存在根目录下的文件夹‘data’中，方便调用。
### 运行主函数
-策略一（base0 + 10类 * 10tasks）对应代码文件main_2.py
```
python main_2.py
```
-策略二（base0 + 5类 * 20tasks）对应代码文件main_3.py
```
python main_3.py
```
-策略三（base50 + 10类 * 5tasks）对应代码文件main_1.py
```
python main_1.py
```
